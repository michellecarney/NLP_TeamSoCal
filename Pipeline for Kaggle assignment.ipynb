{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Kaggle Challenge\n",
    "### Nic Mon and Michelle Carney\n",
    "For this assignment, we used the SciKitLearn tutorial from class, along with the CountVectorizer and SVM documentation. Nic tried to create an SVM predictor but it took too long and kept killing his kernel. Nic also tried pulling out the synsets, but we found that running a synset function on every review would take too long to run in order to be valuable.\n",
    "\n",
    "We decided to go with the logistic regression model because it took the least amount of time to run and we could test more options, and it gave us the best prediction (0.86 right away compared to other methods that gave us 0.30 on the first try). We played around with CountVectorizer and TfidfVectorizer - testing different hypotheses (see below for how well they predicted). We found that if we trained the vecotrizers on unigrams, with the token pattern removing numbers and punctuations and removing stopwords with a df of 2 predicted the test data the best.\n",
    "\n",
    "We found that SVM took too long - we couldn't get it to work without the kernel dying. We also found that reducing the df allowed for more features in the vectorizer, and that increased our accuracy score.\n",
    "\n",
    "Michelle worked on getting the sklearn tutorial from class to work with the data, and getting the CountVectorizer and Tdidfvectorizer to remove stopwords, and for certain strings (using regex), running naive bayes and logistic regression on our training set, and printing the output for the class.\n",
    "\n",
    "Nic worked on training the logistic regression using the whole training set, trying to build synset feature functions, trying to get SVM to work, looking at the text of reviews where our predictors were wrong, and reducing the df to add more features, which yielded a better prediction.\n",
    "\n",
    "We found that SVM took too long - we couldn't get it to work without the kernel dying. We also found that reducing the df allowed for more features in the vectorizer, and that increased our accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"yelp_data_official_training.csv\", low_memory=False, delimiter=\"|\")\n",
    "df_test = pd.read_csv(\"yelp_data_official_test_nocategories.csv\", low_memory=False, delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_index = np.random.permutation(df.index)\n",
    "random_index[:10]\n",
    "df.ix[random_index, ['ID', 'Category', 'Review Text']][:5]\n",
    "df_shuffled = df.ix[random_index, ['ID', 'Category', 'Review Text']]\n",
    "df_shuffled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 48001\n",
      "Columns: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = df_shuffled.shape\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Columns:\", columns)\n",
    "#train_size = round(rows*.6)\n",
    "train_size = round(rows*.9)\n",
    "#dev_size   = round(rows*.2)\n",
    "dev_size   = round(rows*.1)\n",
    "df_train = df_shuffled.loc[:train_size]\n",
    "df_train.shape\n",
    "df_dev = df_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "df_dev.shape\n",
    "df_test = df_shuffled.loc[dev_size+train_size:].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's a bunch of things we tested.....\n",
    "### Let's make a vectorizer to get the array of features! (or let's try a bunch of different vectorizers!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', analyzer=u'word', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##running this got accuracy score of 0.88699093844391208\n",
    "##On the kaggle leaderboard, we got 0.87028\n",
    "###with 1 df on entire training set 0.87833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', analyzer=u'word', stop_words = 'english', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "#testing w removed stop_words\n",
    "##running this got accuracy score of 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b', analyzer=u'word', stop_words = 'english', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##testing only unigrams accuracy score of 0.89636496198312676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b', analyzer=u'word', stop_words = 'english', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##testing only unigrams, removed stopwords accuracy score of 0.89709405270284348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##testing only unigrams, removed stopwords, remove numbers from token pattern accuracy score of 0.89740652015415057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=3)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##testing only unigrams, removed stopwords, remove numbers from token pattern, df=3 accuracy score of 0.8978231434225602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=2)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##testing only unigrams, removed stopwords, remove numbers from token pattern, df=2 accuracy score of 0.89803145505676496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=1)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##testing only unigrams, removed stopwords, remove numbers from token pattern, df=1 accuracy score of 0.89813561087386728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', stop_words=None, analyzer=u'word', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##running this got accuracy score of 0.87751275908759507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b', stop_words=None, analyzer=u'word', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##try only unigrams accuracy score of 0.89303197583585048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=5)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##try only unigrams, remove stop words and numbers, accuracy score of 0.89542755962920528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=7)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##try only unigrams, remove stop words and numbers, df=7 accuracy score of 0.89511509217789809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=2)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##try only unigrams, remove stop words and numbers, df=3 accuracy score of 0.93917300281220706\n",
    "#with full training set, kaggle score 0.87972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 2), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=2)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##try unigrams bigrams, remove stop words and numbers, df=2 accuracy score of 0.94042287261743573\n",
    "## try unigrams bigrams remove stop words and numbers df = 2, train size .6 0.88792834079783356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=1)\n",
    "#Highest predictor of 0.88028\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")\n",
    "##try unigrams bigrams, remove stop words and numbers, df=1 Kaggle score of 0.88028\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make an array of features and test it on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_train_feature_sparse = vec.fit_transform(df_train['Review Text'])\n",
    "arr_train_feature_sparse\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "feature_labels = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_dev_feature_sparse = vec.transform(df_dev[\"Review Text\"])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test this with logistic regression on the dev set and get the accuracy score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88792834079783356"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['Category']) #defining features (from reviews) and passing in Category label\n",
    "logreg_predictions = logreg_model.predict(arr_dev_feature)\n",
    "accuracy_score(df_dev['Category'], logreg_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now let's run this on the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test[\"Review Text\"]) #change to test\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_predictions_test = logreg_model.predict(arr_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is our winning predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import csv, sqlite3\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def read_file():\n",
    "    with open(\"yelp_data_official_training.csv\") as csvfile:\n",
    "        data = pd.read_csv(csvfile, \"r\", delimiter=\"|\")\n",
    "    return data\n",
    "data = read_file()\n",
    "\n",
    "def create_data_sets(data):\n",
    "    random_index = np.random.permutation(data.index)\n",
    "    data_shuffled = data.ix[random_index]\n",
    "    data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    rows, columns = data_shuffled.shape\n",
    "    train_size = round((rows-1)*.75)\n",
    "    dev_size   = round((rows-1)*.25)\n",
    "    train_data = data_shuffled.loc[:train_size]\n",
    "    dev_data = data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "    return train_data, dev_data\n",
    "\n",
    "train_data, dev_data = create_data_sets(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test on our dev data and return where our model got it wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=2)\n",
    "\n",
    "tokenizer = vec.build_tokenizer()\n",
    "\n",
    "train_data = train_data.fillna(\"\")\n",
    "dev_data = dev_data.fillna(\"\")\n",
    "\n",
    "arr_train_feature_sparse = vec.fit_transform(train_data['Review Text'])\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "\n",
    "arr_dev_feature_sparse = vec.transform(dev_data['Review Text'])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "logreg_model = log_reg.fit(arr_train_feature, train_data['Category'])\n",
    "logreg_predictions = logreg_model.predict(arr_dev_feature)\n",
    "\n",
    "for real, pred, text in zip(dev_data['Category'], logreg_predictions, dev_data['Review Text']):\n",
    "    if real != pred:\n",
    "        print(real, pred)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use our whole training data to train the log reg and predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test_file():\n",
    "    with open(\"yelp_data_official_test_nocategories.csv\") as csvfile:\n",
    "        data = pd.read_csv(csvfile, \"r\", delimiter=\"|\")\n",
    "    return data\n",
    "\n",
    "test_data = read_test_file()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=u'word', stop_words = 'english', min_df=1)\n",
    "#Highest predictor of 0.88028\n",
    "\n",
    "tokenizer = vec.build_tokenizer()\n",
    "\n",
    "data = data.fillna(\"\")\n",
    "test_data = test_data.fillna(\"\")\n",
    "\n",
    "arr_train_feature_sparse = vec.fit_transform(data['Review Text'])\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(test_data['Review Text'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "logreg_model = log_reg.fit(arr_train_feature, data['Category'])\n",
    "logreg_predictions_test = logreg_model.predict(arr_test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a CSV to submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test['Category'] = logreg_predictions_test\n",
    "cols = [\"ID\", \"Category\"]\n",
    "finalsubmission = df_test[cols]\n",
    "finalsubmission.to_csv('yelp_data_official_test_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>11995</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>11996</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11997</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>11998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>11999</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Category\n",
       "11995  11995         4\n",
       "11996  11996         4\n",
       "11997  11997         4\n",
       "11998  11998         4\n",
       "11999  11999         4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalsubmission.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
